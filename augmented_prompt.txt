how about now? Here's source code from parts of the user's code repo that like are helpful: 


 

__________________________Source code for streamlit_app.py: 

 import streamlit as st
import asyncio
import os
import time
from dotenv import load_dotenv

# Custom functions 
from repo_visualizer import visualiserepo
from stream_response import stream_response
from custom_html import custom_html
from gpt_response import gpt_response
from create_prompt_from_settings import create_prompt_from_settings
from render_message import render_message
from handle_streamed_input import handle_streamed_input
from check_and_delete_file_on_first_load import check_and_delete_file_on_first_load
from analyze_repo import create_json_of_interactions, read_code
from module_for_main import call_initialisation, set_page_config, create_custom_style, set_loggers
from add_context_to_user_prompt import add_context_to_user_prompt

# FUNCTIONS IN THIS FILE:
# main
# send_message
# call_initialisation
# get_json_of_interactions
# handle_button_click

def send_message(settings, github_link=None, repo_json=None):
    prompt = st.chat_input('"Make a webcrawler that avoids bot catchers" | "Speed up my code"')
    # check whether repo_json exists
    if prompt and repo_json is not None and github_link is not None:
        st.toast('Reading through all of your code', icon="üìñ")
        time.sleep(1)
        st.toast('Figuring out what\'s relevant', icon="ü•í")
        print('\n\n\n\n\n\n\n\n Entering augmented_prompt\n\n\n\n\n\n\n\n\n\n')
        prompt_augmentation = add_context_to_user_prompt(repo_json, github_link, prompt)
        print('\n\n\n\n\n\n\n\n Leaving augmented_prompt\n\n\n\n\n\n\n\n\n\n')
        with open('augmented_prompt.txt', 'w') as file:
            file.write(prompt + prompt_augmentation)
        print('\n\n\n\n\n\n\n\n writing augmented_prompt.txt to a file\n\n\n\n\n\n\n\n\n\n')
        st.toast('Done', icon="üõéÔ∏è")
        asyncio.run(handle_streamed_input(prompt, settings, prompt_augmentation))
        st.toast("Done", icon = "üç™")
    elif prompt and repo_json is None:
        st.toast('**Note** | You can add your repo link, and I\'ll consider it all while answering.', icon="ü•∑")
        asyncio.run(handle_streamed_input(prompt, settings))    

def get_json_of_interactions(github_link, repo_json=None):
    if repo_json is None:
        st.toast('Analyzing your codebase', icon="üõ∞Ô∏è")
        repo_json = create_json_of_interactions(github_link)
        if repo_json is None:
            st.toast('Is the link correct?', icon="üêî")
        else:
            st.toast('Finished! Tell me what to do.', icon="ü•í")
    return repo_json

def handle_button_click(prompt, settings):
    st.session_state.show_buttons = False # Ensure the button is hidden
    asyncio.run(handle_streamed_input(prompt, settings))

def main():    
    load_dotenv()
    call_initialisation()    
    set_page_config()
    set_loggers()
    custom_style = create_custom_style()
    st.markdown(custom_style, unsafe_allow_html=True)    
    left_column, middle_column, right_column = st.columns([1, 4, 1])

    with middle_column:
        settings = custom_html()
        st.write('### **Give it a try**')
    
        if 'github_link' not in st.session_state:
            st.session_state.github_link = None
        if 'repo_json' not in st.session_state:
            st.session_state.repo_json = None

        on = st.toggle("Tough problem? **Boost.**")
        if on:
            st.session_state.iterations = 1
            st.toast("**Boost activated** for superior problem solving", icon = "ü™º")
            
        github_link = st.chat_input('url to your github repo (optional)')
        if github_link:
            st.session_state.github_link = github_link
            if st.session_state.repo_json is None:
                st.session_state.repo_json = get_json_of_interactions(github_link)
            st.write(f'GitHub Link: {st.session_state.github_link}')
            
        send_message(settings, st.session_state.github_link, st.session_state.repo_json)
        
        check_and_delete_file_on_first_load()
        output_image_path = "codebase_graph.png"
        if os.path.exists(output_image_path):
            st.image(output_image_path)
        else:
            st.image("https://i.imgur.com/k9YDfOV.png")
            st.write('*Note*: Currently only visualisation, and that is buggy. Loading the context into the conversation background will be here in the next days.')
        st.divider()

    with right_column:
        st.write(' ')

if __name__ == "__main__":
    main() 



_____________________________Source code for repo_visualizer.py: 

 import os
import shutil
import json
import ast
import mimetypes
import logging
from git import Repo, GitCommandError
import pydot
import streamlit as st
from collections import defaultdict

logging.basicConfig(level=logging.DEBUG)

def clone_repo(repo_url, clone_dir='cloned_repo'):
    logging.debug(f"Attempting to clone repository: {repo_url}")
    try:
        if os.path.exists(clone_dir):
            logging.warning(f"Directory {clone_dir} already exists, deleting...")
            shutil.rmtree(clone_dir)
        logging.info(f"Cloning repository {repo_url} into {clone_dir}")
        Repo.clone_from(repo_url, clone_dir)
        return clone_dir
    except GitCommandError as e:
        logging.error(f"Failed to clone repository: {e}")
        return None
    except Exception as e:
        logging.error(f"Unexpected error: {e}")
        return None

def is_text_file(filepath):
    """Check if the file is a text file or specific recognized type."""
    mime_type, _ = mimetypes.guess_type(filepath)
    if mime_type and mime_type.startswith('text'):
        return True
    file_ext = os.path.splitext(filepath)[1]
    return file_ext in ['.py', '.html', '.json', '.md', '.txt', '.yaml', '.yml', '.js', '.css']

def extract_ast_details(tree):
    logging.debug("Extracting AST details")
    imports = []
    functions = []
    classes = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):
            imports.append(ast.dump(node))
        elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            func_start_line = node.lineno
            func_end_line = max((n.lineno for n in ast.walk(node) if hasattr(n, 'lineno')), default=func_start_line)
            loc = func_end_line - func_start_line + 1

            functions.append({
                'name': node.name,
                'args': [arg.arg for arg in node.args.args],
                'calls': [
                    n.func.id for n in ast.walk(node)
                    if isinstance(n, ast.Call) and hasattr(n.func, 'id')
                ],
                'loc': loc
            })
        elif isinstance(node, ast.ClassDef):
            classes.append({
                'name': node.name,
                'methods': [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
            })

    return imports, functions, classes

def extract_codebase_structure(root_dir, temp_file='temp_codebase.json'):
    logging.debug(f"Extracting codebase structure from directory: {root_dir}")
    codebase = defaultdict(lambda: {'imports': [], 'functions': [], 'classes': []})

    if os.path.exists(temp_file):
        os.remove(temp_file)
    
    with open(temp_file, 'w') as tempf:
        for root, _, files in os.walk(root_dir):
            for file in files:
                file_path = os.path.join(root, file)
                if is_text_file(file_path):
                    logging.info(f"Parsing file: {file_path}")
                    with open(file_path, 'r', encoding='utf-8') as f:
                        try:
                            tree = ast.parse(f.read(), filename=file_path)
                            imports, functions, classes = extract_ast_details(tree)
                            file_details = {
                                'imports': imports,
                                'functions': functions,
                                'classes': classes
                            }
                            json.dump({file_path: file_details}, tempf)
                            tempf.write('\n')  # Separate each entry by a newline
                        except SyntaxError as se:
                            logging.warning(f"SyntaxError in file {file_path}: {se}")
                        except Exception as e:
                            logging.error(f"Failed to parse {file_path}: {e}")

def visualize_codebase_structure(temp_file='temp_codebase.json', output_file='codebase_graph.png'):
    logging.debug("Visualizing codebase structure")
    graph = pydot.Dot(graph_type='digraph', layout='neato', overlap=False, splines=True, resolution=120)
    graph.set_graph_defaults(size="12,12!", dpi="780")

    file_nodes = {}
    function_nodes = {}
    class_nodes = {}
    folder_clusters = defaultdict(list)

    def add_node_and_edges(file, details):
        if file not in file_nodes:
            file_node = pydot.Node(file, shape='box', style='filled', fillcolor='lightgrey', fontsize=16, fontcolor='black', width=3.0, height=0.3)
            graph.add_node(file_node)
            file_nodes[file] = file_node
            folder = os.path.dirname(file)
            folder_clusters[folder].append(file_node)

        for cls in details['classes']:
            cls_label = f"{cls['name']} (Class)"
            if cls_label not in class_nodes:
                class_node = pydot.Node(cls_label, shape='ellipse', style='filled', fillcolor='blue', fontcolor='white', fontsize=20)
                graph.add_node(class_node)
                class_nodes[cls_label] = class_node

            graph.add_edge(pydot.Edge(file_nodes[file], class_nodes[cls_label], color='blue', penwidth=2.0))

            for method in cls['methods']:
                method_label = f"{cls['name']}.{method} (Method)"
                method_node = pydot.Node(method_label, shape='ellipse', style='filled', fillcolor='green', fontcolor='white', fontsize=20)
                graph.add_node(method_node)
                graph.add_edge(pydot.Edge(class_nodes[cls_label], method_node, color='green', penwidth=2.0))

        for func in details['functions']:
            func_label = func['name']
            size = max(func['loc'] / 10.0, 1.0)
            if func_label not in function_nodes:
                function_node = pydot.Node(func_label, shape='circle', style='filled', fillcolor='black', fontcolor='white', fontsize=30, width=size, height=size)
                graph.add_node(function_node)
                function_nodes[func_label] = function_node
            else:
                function_node = function_nodes[func_label]

            graph.add_edge(pydot.Edge(file_nodes[file], function_node, color='purple', penwidth=2.0))

            for call in func['calls']:
                call_label = call
                if call_label not in function_nodes:
                    call_node = pydot.Node(call_label, shape='circle', style='filled', fillcolor='black', fontcolor='white', fontsize=20, width=size, height=size)
                    graph.add_node(call_node)
                    function_nodes[call_label] = call_node
                else:
                    call_node = function_nodes[call_label]

                graph.add_edge(pydot.Edge(function_node, call_node, label=f"args: {', '.join(func['args'])}", labeldistance=2, labelangle=45, color='purple', penwidth=2.0))

    with open(temp_file, 'r') as tempf:
        for line in tempf:
            try:
                file_details = json.loads(line)
                for file, details in file_details.items():
                    add_node_and_edges(file, details)
                    if len(graph.get_node_list()) % 100 == 0:  # Periodically write to file to free up memory
                        try:
                            graph.write_png(output_file)
                            logging.info(f"Intermediate visualization saved to {output_file}")
                            graph.del_node(file_nodes[file])
                            for cls in details['classes']:
                                graph.del_node(class_nodes[f"{cls['name']} (Class)"])
                            for func in details['functions']:
                                graph.del_node(function_nodes[func['name']])
                        except Exception as e:
                            logging.error(f"Failed to generate intermediate graph: {e}")
            except json.JSONDecodeError as e:
                logging.error(f"Failed to parse JSON line: {e}")

    dot_data = graph.to_string()
    logging.debug(f"DOT file content:\n{dot_data}")

    try:
        graph.write_png(output_file)
        logging.info(f"Final visualization saved to {output_file}")
    except Exception as e:
        logging.error(f"Failed to generate final graph: {e}")

def visualiserepo(github_link):
    logging.debug(f"Starting visualization for repo: {github_link}")
    clone_dir = clone_repo(github_link)
    if clone_dir:
        extract_codebase_structure(clone_dir)        
        running_locally = not 'STREAMLIT_SERVER' in os.environ
        if running_locally:
            visualize_codebase_structure()        
        return 'codebase_graph.png'
    else:
        logging.error("Cloning failed, skipping visualization.")
 



_____________________________Source code for handle_streamed_input.py: 

 import streamlit as st
from create_prompt_from_settings import create_prompt_from_settings
from render_message import render_message
from stream_assistant_response import stream_assistant_response
from stream_instructor_response import stream_instructor_response
from add_context_to_user_prompt import add_context_to_user_prompt  # Import the function here

async def handle_streamed_input(user_input, settings, prompt_augmentation=None, repo_json=None, github_link=None):
    # Append user input
    st.session_state.messages.append({"role": "user", "content": user_input, "displayed": False})
    st.session_state.run = True

    # Generate and append the augmented prompt if not already done
    if repo_json is not None and github_link is not None:
        # Generate and append the augmented prompt
        prompt_augmentation = add_context_to_user_prompt(repo_json, github_link, user_input)
        st.session_state.messages.append({"role": "system", "content": prompt_augmentation, "displayed": False})

    if prompt_augmentation is not None:
        st.session_state.messages.append({"role": "system", "content": prompt_augmentation, "displayed": False})

    # Display user message
    user_input_placeholder = st.empty()
    render_message(user_input_placeholder, "You", user_input, no_expander=True)

    # Placeholders for responses
    assistant_placeholders = []
    instructor_placeholders = []
    progress_placeholders = []

    for _ in range(st.session_state.iterations):
        # Add placeholders for responses
        assistant_placeholder = st.empty()
        assistant_placeholders.append(assistant_placeholder)

        progress_placeholder = st.empty()
        progress_placeholders.append(progress_placeholder)

        instructor_placeholder = st.empty()
        instructor_placeholders.append(instructor_placeholder)

        # Get assistant response
        await stream_assistant_response(st.session_state.messages, assistant_placeholder)

        doing_message = "üîÅ ü§ñ Improving my code"
        render_message(progress_placeholder, "Doing", doing_message, no_expander=True)

        # Get instructor response
        await stream_instructor_response(st.session_state.messages, instructor_placeholder)

    # Final response
    final_response_placeholder = st.empty()
    await stream_assistant_response(st.session_state.messages, final_response_placeholder, no_expander=True)

    st.session_state.run = False
    st.session_state.show_buttons = False 



_____________________________Source code for add_context_to_user_prompt.py: 

 import streamlit as st
from analyze_repo import read_code
import json
from gpt_response import gpt_response
from get_text_between_tags import get_text_between_tags

def create_instructions_for_LLM(json_str, user_prompt):
    instructions_for_LLM = f"**Here's a Json that represents my codebase.**: \n\n {json_str}"
    instructions_for_LLM += ''' 
    
    
    
    
    _________________________
    What I want to accomplish:
    ''' 
    instructions_for_LLM += f"{user_prompt}"
        
    instructions_for_LLM += ''' 
    


    ________________________
    Please don't write any code unless I explicitly ask you to. Before we start with all that, please choose up to 5 files that you'd like me to show to another LLm that will then be tasked with solving my task.
    I'll immediately and without looking run your answer through a string processing algorithm that will look for filenames, and return their source code to you. 
    
    For this to work, it's important to stick with a convention:
    Each filename has to go inside of html tags like this:
    
    <f>readme.rtfd</f>
    <f>main.py</f>
    <f>transcribe_audio.py</f>
    
    (Names from my example above are made up. Make sure to only provide filenames present in the json above). If there's a readme, always look at that.
        
    Additionally, since you know the bigger picture of my codebase a little, give tips to the LLM pertaining to that, and place it inside of brackets:
    <tips> TIPS </tips>. 
    
    Your tips should only be a couple paragraphs long.    
    '''
    return instructions_for_LLM

def create_list_of_filenames(input_string):
    start_tag = "<f>"
    end_tag = "</f>"
    list_of_filenames = []
    seen_filenames = set()
    start_index = 0

    while True:
        start_loc = input_string.find(start_tag, start_index)
        if start_loc == -1:
            break
        end_loc = input_string.find(end_tag, start_loc + len(start_tag))
        if end_loc == -1:
            break
        filename = input_string[start_loc + len(start_tag):end_loc]
        if filename not in seen_filenames:
            list_of_filenames.append(filename)
            seen_filenames.add(filename)
        start_index = end_loc + len(end_tag)
    
    return list_of_filenames

        
def add_context_to_user_prompt(repo_json, github_link, user_prompt):	
    
    # Have gpt look at the json, and return a list with up to n entries of filanmes.                
    json_str = json.dumps(repo_json, indent=4)    
    instructions_for_LLM = create_instructions_for_LLM(json_str, user_prompt)
        
    response = gpt_response(instructions_for_LLM)
    response = response.replace("cloned_repo/", "") # removes "cloned_repo/"
    list_of_filenames = create_list_of_filenames(response)
    for filename in list_of_filenames:
        st.toast("Highly pertinent | " + filename, icon = "ü™º")

    pertinent_source_code = '\n\n__________________________'
    # Loop over the list and append the code for the chosen files and iteratively append them to context
    tips = get_text_between_tags(response, "tips")
    
    for filename in list_of_filenames:
        nLOC, source_code_for_this_file = read_code(repo_json, filename, github_link)
        pertinent_source_code += f"Source code for {filename}: \n\n {source_code_for_this_file} \n\n\n\n_____________________________"        

    # Append the codestring to json_str     
    augmented_prompt = f"Here's source code from parts of the user's code repo that like are helpful: \n\n\n {pertinent_source_code}\n\n\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî" 
    augmented_prompt += f"Some tips: \n\n\n {tips}."
    
    print("________________\n\n\n\n\LLM RESPONSE: \n\n\n\n" + response + "\n\n\n\n\n\n\n\n")
    print("________________\n\n\n\n\INSTRUCTIONS FOR LLM: \n\n\n\n" + instructions_for_LLM + "\n\n\n\n\n\n\n\n")
    print("________________\n\n\n\n\FILENAMES: \n\n\n\n" + str(list_of_filenames) + "\n\n\n\n\n\n\n\n")
    print("________________\n\n\n\n\TIPS: \n\n\n\n" + tips + "\n\n\n\n\n\n\n\n")
        
    print("________________\n\n\n\n\nAUGMENTED PROMPT: \n\n\n\n" + augmented_prompt + "\n\n\n\n\n\n\n\n")
    return augmented_prompt 



_____________________________Source code for gpt_response.py: 

 import time
import openai
import os
from dotenv import load_dotenv


def gpt_response(prompt):        
    load_dotenv() 
    openai.api_key = os.getenv("OPENAI_API_KEY")
    
    max_retry = 3
    retry_counter = 0
    modelversion = 'gpt-4o'
                    
    message_history = []

    if isinstance(prompt, str):        
        message_history.append({"role": "user", "content": prompt})

    elif isinstance(prompt, list):
        for message in prompt:
            if "content" in message:                                
                message_history.append(message)

    else:
        print(f"Data: {prompt}")
        print(f"Type : {type(prompt)}")
        print("Invalid prompt type. It must be a string or list of message objects.")
        return None
                    
    while retry_counter <= max_retry:
        try:
            response = openai.chat.completions.create( # "chat.completions" needs to stay this way
                model=modelversion,
                messages=message_history,
                stream=False,
            )
            return response.choices[0].message.content
        
        except Exception as e:
            if retry_counter < max_retry:
                print(f"   *** An error occurred ({str(e)}). Trying again in 300ms. ***")
                time.sleep(0.3)
                retry_counter += 1
            else:
                print("   *** Retry limit reached. Ending execution. ***")
                return None

# Test function
def test():
    prompt = 'How are ya?'
    response = gpt_response(prompt)
    print(response)

if __name__ == "__main__":
    test()
 



_____________________________


‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚ÄîSome tips: 


 
The `streamlit_app.py` file is likely the main entry point for your application, handling initial setup and user interactions. The `repo_visualizer.py` file contains functionalities related to visualizing the codebase, which might be crucial for understanding the structure. The `handle_streamed_input.py` file deals with processing user inputs and streaming responses, which might interact with multiple modules. The `add_context_to_user_prompt.py` file appears to be important for augmenting user prompts based on the repository's context. Finally, the `gpt_response.py` file handles interactions with the GPT model, which seems central to fetching responses from the AI model.
.